[0m[[0minfo[0m] [0mRunning org.apache.spark.recommendation.Recommendation [0m
[0m[[31merror[0m] [0mUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties[0m
[0m[[31merror[0m] [0m15/04/25 19:36:26 INFO SparkContext: Running Spark version 1.3.0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:26 WARN Utils: Your hostname, anisha-Inspiron-1564 resolves to a loopback address: 127.0.0.1; using 10.24.28.131 instead (on interface wlan0)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address[0m
[0m[[31merror[0m] [0m15/04/25 19:36:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[0m
[0m[[31merror[0m] [0m15/04/25 19:36:27 INFO SecurityManager: Changing view acls to: hduser[0m
[0m[[31merror[0m] [0m15/04/25 19:36:27 INFO SecurityManager: Changing modify acls to: hduser[0m
[0m[[31merror[0m] [0m15/04/25 19:36:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hduser); users with modify permissions: Set(hduser)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Slf4jLogger: Slf4jLogger started[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Remoting: Starting remoting[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@anisha-inspiron-1564.utdallas.edu:33401][0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Utils: Successfully started service 'sparkDriver' on port 33401.[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO SparkEnv: Registering MapOutputTracker[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO SparkEnv: Registering BlockManagerMaster[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO DiskBlockManager: Created local directory at /tmp/spark-ef41ce20-8b46-4501-9744-eecee9e5525c/blockmgr-c7fa4ade-5c04-45bd-ad3f-844885614aa2[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO MemoryStore: MemoryStore started with capacity 462.8 MB[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO HttpFileServer: HTTP File server directory is /tmp/spark-916bb098-bfba-4c93-af8e-db960754d104/httpd-8a658866-38b4-4dc3-87f8-9dacefaea011[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO HttpServer: Starting HTTP Server[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Server: jetty-8.y.z-SNAPSHOT[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO AbstractConnector: Started SocketConnector@0.0.0.0:55311[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO Utils: Successfully started service 'HTTP file server' on port 55311.[0m
[0m[[31merror[0m] [0m15/04/25 19:36:28 INFO SparkEnv: Registering OutputCommitCoordinator[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO Server: jetty-8.y.z-SNAPSHOT[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO SparkUI: Started SparkUI at http://anisha-inspiron-1564.utdallas.edu:4040[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO Executor: Starting executor ID <driver> on host localhost[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@anisha-inspiron-1564.utdallas.edu:33401/user/HeartbeatReceiver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO NettyBlockTransferService: Server created on 57281[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO BlockManagerMaster: Trying to register BlockManager[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO BlockManagerMasterActor: Registering block manager localhost:57281 with 462.8 MB RAM, BlockManagerId(<driver>, localhost, 57281)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO BlockManagerMaster: Registered BlockManager[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO MemoryStore: ensureFreeSpace(132516) called with curMem=0, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 129.4 KB, free 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(18512) called with curMem=132516, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.1 KB, free 462.6 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57281 (size: 18.1 KB, free: 462.8 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO SparkContext: Created broadcast 0 from textFile at Recommendation.scala:49[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(132548) called with curMem=151028, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 129.4 KB, free 462.5 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(18512) called with curMem=283576, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.1 KB, free 462.5 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:57281 (size: 18.1 KB, free: 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO SparkContext: Created broadcast 1 from textFile at Recommendation.scala:50[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(132548) called with curMem=302088, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 129.4 KB, free 462.4 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(18512) called with curMem=434636, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.1 KB, free 462.3 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:57281 (size: 18.1 KB, free: 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO SparkContext: Created broadcast 2 from textFile at Recommendation.scala:51[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO FileInputFormat: Total input paths to process : 1[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO SparkContext: Starting job: collect at Recommendation.scala:56[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Got job 0 (collect at Recommendation.scala:56) with 2 output partitions (allowLocal=false)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Final stage: Stage 0(collect at Recommendation.scala:56)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Parents of final stage: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Missing parents: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[7] at map at Recommendation.scala:56), which has no missing parents[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(2944) called with curMem=453148, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 462.3 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: ensureFreeSpace(2111) called with curMem=456092, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 462.3 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:57281 (size: 2.1 KB, free: 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:839[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MapPartitionsRDD[7] at map at Recommendation.scala:56)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1323 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1323 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/ratings.dat:12297065+12297066[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/ratings.dat:0+12297065[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id[0m
[0m[[31merror[0m] [0m15/04/25 19:36:30 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1792 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1820 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2009 ms on localhost (1/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2025 ms on localhost (2/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Stage 0 (collect at Recommendation.scala:56) finished in 2.046 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool [0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Job 0 finished: collect at Recommendation.scala:56, took 2.145862 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO FileInputFormat: Total input paths to process : 1[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO SparkContext: Starting job: collect at Recommendation.scala:65[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Got job 1 (collect at Recommendation.scala:65) with 2 output partitions (allowLocal=false)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Final stage: Stage 1(collect at Recommendation.scala:65)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Parents of final stage: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Missing parents: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[10] at map at Recommendation.scala:65), which has no missing parents[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO MemoryStore: ensureFreeSpace(2712) called with curMem=458203, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 2.6 KB, free 462.3 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO MemoryStore: ensureFreeSpace(1999) called with curMem=460915, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1999.0 B, free 462.3 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:57281 (size: 1999.0 B, free: 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:839[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[10] at map at Recommendation.scala:65)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1322 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1322 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/movies.dat:0+85654[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/movies.dat:85654+85654[0m
[0m[[31merror[0m] [0m15/04/25 19:36:32 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 80501 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 79088 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 234 ms on localhost (1/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Stage 1 (collect at Recommendation.scala:65) finished in 0.423 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Job 1 finished: collect at Recommendation.scala:65, took 0.440331 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 366 ms on localhost (2/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool [0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO FileInputFormat: Total input paths to process : 1[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO SparkContext: Starting job: saveAsTextFile at Recommendation.scala:70[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Got job 2 (saveAsTextFile at Recommendation.scala:70) with 2 output partitions (allowLocal=false)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Final stage: Stage 2(saveAsTextFile at Recommendation.scala:70)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Parents of final stage: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Missing parents: List()[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Submitting Stage 2 (MapPartitionsRDD[12] at saveAsTextFile at Recommendation.scala:70), which has no missing parents[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: ensureFreeSpace(227480) called with curMem=462914, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 222.1 KB, free 462.1 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: ensureFreeSpace(169217) called with curMem=690394, maxMem=485260001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 165.3 KB, free 462.0 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:57281 (size: 165.3 KB, free: 462.6 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:839[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Submitting 2 missing tasks from Stage 2 (MapPartitionsRDD[12] at saveAsTextFile at Recommendation.scala:70)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, PROCESS_LOCAL, 1340 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, PROCESS_LOCAL, 1340 bytes)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/similarity-matrix/part-00000:3986389+3986390[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO HadoopRDD: Input split: file:/home/hduser/recommendation/src/main/scala/similarity-matrix/part-00000:0+3986389[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO FileOutputCommitter: Saved output of task 'attempt_201504251936_0002_m_000001_5' to file:/home/hduser/recommendation/movieIDNameList/_temporary/0/task_201504251936_0002_m_000001[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO SparkHadoopWriter: attempt_201504251936_0002_m_000001_5: Committed[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 1792 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO FileOutputCommitter: Saved output of task 'attempt_201504251936_0002_m_000000_4' to file:/home/hduser/recommendation/movieIDNameList/_temporary/0/task_201504251936_0002_m_000000[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO SparkHadoopWriter: attempt_201504251936_0002_m_000000_4: Committed[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1792 bytes result sent to driver[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 244 ms on localhost (1/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 247 ms on localhost (2/2)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Stage 2 (saveAsTextFile at Recommendation.scala:70) finished in 0.248 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool [0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO DAGScheduler: Job 2 finished: saveAsTextFile at Recommendation.scala:70, took 0.321759 s[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManager: Removing broadcast 5[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManager: Removing block broadcast_5_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: Block broadcast_5_piece0 of size 169217 dropped from memory (free 484569607)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:57281 in memory (size: 165.3 KB, free: 462.7 MB)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO BlockManager: Removing block broadcast_5[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO MemoryStore: Block broadcast_5 of size 227480 dropped from memory (free 484797087)[0m
[0m[[31merror[0m] [0m15/04/25 19:36:33 INFO ContextCleaner: Cleaned broadcast 5[0m
